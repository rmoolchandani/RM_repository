---
title: "Machine Learning Assignment#4"
author: "Rakhee Moolchandani"
date: "11/01/2020"
output: 
  pdf_document: default
  html_document: default
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#load all the required libraries
```{r,message=FALSE}
library(readr)
library(gmodels)
library(ISLR)
library(dplyr)
library(tidyverse)
library(factoextra)
library(caret)
library(compareGroups)
library(data.table)
library(fpc)
library(ggplot2)
library(GGally)
```


# Import the Universities Data
```{r}
#Read the data set
UniversityData <- read.csv("Universities.csv")
#Show the first few rows of the data set  
head(UniversityData)
```

# a)Remove all records with missing measurements from the dataset. Also, remove all the categorical values
```{r}
#Remove all the missing values from the data set by using na.omit
UniData <- na.omit(UniversityData)
#Remove all the categorical variables from the data set
UniData1 <- UniData[, c(-1, -2, -3)]
```

# b)For all the continuous measurements, run K-Means clustering. Make sure to normalize the measurements. How many clusters seem reasonable for describing these data? What was your optimal K?
```{r}
#Scale the data set
ScaledData <- scale(UniData1)
#Look at the summary of the scaled data
summary(ScaledData)
#Calculate the distance for the scaled data  
distance <- get_dist(ScaledData)
#plot the distance using fviz_dist
fviz_dist(distance,gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```


## The above graph shows the distance between variables. Let us now determine the clusters.


# Determining Optimal Clusters:
## K Means clustering is a simple algorithm used to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. K-means clustering requires that you specify in advance the number of clusters to extract. A plot of the total within-groups sums of squares against the number of clusters in a k-means solution can be helpful. A bend in the graph can suggest the appropriate number of clusters.
## Below are the methods to determine the optimal number of clusters:
## Elbow method
## Silhouette method


# Let us use an “elbow chart” to determine k
```{r}
set.seed(123)
fviz_nbclust(ScaledData, kmeans, method = "wss")
```


## The chart shows that the elbow point 3 provides the best value for k. While WSS will continue to drop for larger values of k, we have to make the trade off between over fitting, i.e., a model fitting both noise and signal, to a model having bias. Here, the elbow point provides that compromise where WSS, while still decreasing beyond k = 3, decreases at a much smaller rate. In other words, adding more clusters beyond 3 brings less improvement to cluster homogeneity.


# Now, Let us also apply the Silhouette Method to determine the number of clusters
```{r}
fviz_nbclust(ScaledData, kmeans, method = "silhouette")
```


## Again, we see that 3 is the ideal number of clusters. Here we look for large values for the Silhouette Width (Y Axis)


# Run the Kmeans algorithm for clustering
```{r}
#We will choose the value of k = 3 as observed in the above methods, number of restarts =15 
k3 <- kmeans(ScaledData, centers = 3, nstart = 15)
#output the centers 
k3$centers 
#No. of Universities in each cluster i.e. the size of the clusters
k3$size 
#Identify the cluster of the 325th observation as an example
k3$cluster[325] 
#Visualize the output
fviz_cluster(k3, data = ScaledData) 
#Add the cluster column in the University Data set
UniData1$Cluster = k3$cluster
# Label the Clusters
UniData1$Cluster <- factor(UniData1$Cluster,levels = c(1,2,3),labels = c("Cluster1","Cluster2" , "Cluster3"))
#Plot the graph cluster wise
ggparcoord(UniData1, column= 1:17, groupColumn = "Cluster")
```


## There are 3 clusters formed with size 275, 150 and 46. 
## Based on the graph plotted, we can identify some cluster groupings here:
### 1) Orange samples (cluster 1) with a high proportion of student faculty ratio and lower number of faculties with PHD.
### 2) Green samples (cluster 2), some of which have a high number of in state ans out of state tution and low portion of student faculty ratio.
### 3) Blue samples (cluster 3) with high number of students enrolled and Full time graduates and low in state tution 


# c)Compare the summary statistics for each cluster and describe each cluster in this context
```{r}
#Use comparegroups function to compare the data set cluster wise
comparegroups.main = compareGroups(formula = Cluster~., data = UniData1[,c(1:18)])
comparegroups.main = createTable(x= comparegroups.main,show.all = T)
#View the descriptive summary by Cluster
comparegroups.main
#Plot the summary statistics for each cluster
#Plot an empty scatter plot
plot(c(0), xaxt = 'n', ylab = "", type = "l", ylim = c(min(k3$centers), max(k3$centers)), xlim = c(0, 18))
#Label x-axes
axis(1, at = c(1:17), labels = colnames(k3$centers))
#Plot centroids
for (i in c(1:3))
lines(k3$centers[i,], lty = i, lwd = 2, col = ifelse(i %in% c(1, 2, 3), "black", "dark grey"))
#Name clusters
text(x = 0.5, y = k3$centers[, 1], labels = paste("Cluster", c(1:3)))
```


## We can now more clearly see the variation across the variables for each of the clusters found by the k-means algorithm.
## The graph and the comparison table interprets that most of the variables are high in cluster 3 and 2 and none of the variables is high in Cluster 1.



# d)Use the categorical measurements that were not used in the analysis (State and Private/Public) to characterize the different clusters. Is there any relationship between the clusters and the categorical information?
```{r, message= FALSE}
#Combine the University name, State and Private/Public columns along with the clusters
CombinedData <- cbind(UniData[,c(1:3)],k3$cluster)
#Label the column names
colnames(CombinedData) <- c("College.Name","State","Public..1...Private..2.","Clusters")
#Label the clusters with names like Cluster1, Cluster2, etc.
CombinedData$Public..1...Private..2. <- factor(CombinedData$Public..1...Private..2.)
CombinedData$Clusters <- factor(CombinedData$Clusters,levels = c(1,2,3),labels = c("Cluster1","Cluster2" , "Cluster3"))
#See the first few rows of the Combined data
head(CombinedData)
#Find all the Public Universities in each cluster
PubUni <- CombinedData %>% group_by(Clusters) %>% filter(Public..1...Private..2. ==1) %>%summarise(PublicUniverity =n())
#Find all the Private Universities in each cluster
PrivUni <- CombinedData %>% group_by(Clusters) %>% filter(Public..1...Private..2. ==2) %>% summarise(PrivateUniverity =n())
#Combine the output
PubPriv <- cbind(PubUni,PrivUni[,2])
#Display the number of private and public universities in each cluster
PubPriv
#Plot the graph to show the number of public and private universities in each State for every cluster
ggplot(CombinedData, aes(x=Public..1...Private..2., y=State, color=Clusters)) + geom_point()
```


## From the plot and from the table, it can be determined that:
### The Cluster 1 has more portion of private Universities and less portion of public Universities.
### The Cluster 2 has more Private Universities.
### And the Cluster 3 has more Public Universities.



# e)What other external information can explain the contents of some or all of these clusters?
## The external factors that can impact the contents of the clusters could be following:
### 1) Climate : Climatic conditions of the University location.
### 2) Safety : Schools that have high levels of violence and poor student-teacher relations are considered not safe.
### 3) Recognization: In order to promote success, success needs to be recognized. 
### 4) Enviroment: Interactions between adults and students, environmental factors, academic performance and feelings of trust and respect among educational stakeholders.



# f)Consider Tufts University, which is missing some information. Compute the Euclidean distance of this record from each of the clusters that you found above (using only the measurements that you have). Which cluster is it closest to? Impute the missing values for Tufts by taking the average of the cluster on those measurements.
## Apparently, in clustering in which the distance measure is Euclidean distance, the data must be first normalized or standardized to prevent the covariate with the highest variance from driving the clustering. Why is this? 
## It depends on the data. And actually it has nothing to do with clustering, but with the distance function. The problem is when we have mixed attributes. For example, we have data on persons. Weight in grams and shoe size. Shoe sizes differ very little, while the differences in body mass (in grams) are much much larger. Similarly, we have this University data set. We just cannot compare 1 g and 1 shoe size difference.Usually in these cases, Euclidean distance just does not make sense. But it may still work, in many situations if we normalize our data.

```{r}
#Read the Tufts University Data from the University data
TuftUni <- UniversityData[UniversityData$College.Name=="Tufts University",]
#Save the University data (remove categorical values and missing values)
UniData2 <- UniData[,-c(1,2,3)]
#Scale the University data and The tufts University data
norm.values <- preProcess(UniData2,method=c("center","scale"))
UniData2 <- predict(norm.values,UniData2)
TuftUni <- predict(norm.values,TuftUni)
#View the Tufts University data
TuftUni
# Compute the Euclidean Distance
dist(rbind(TuftUni[, -c(1,2,3)], k3$centers[1,]))
dist(rbind(TuftUni[, -c(1,2,3)], k3$centers[2,]))
dist(rbind(TuftUni[, -c(1,2,3)], k3$centers[3,]))
#Cluster 2 is closest 
#Impute the missing value
TuftUni$X..PT.undergrad <- k3$centers[2,7]
TuftUni
####
```

## The tufts University is closest to the Cluster 2. The mean value of the column X..PT.undergrad for the cluster 2 is imputed to the missing value of that column for the Tuft University data.

