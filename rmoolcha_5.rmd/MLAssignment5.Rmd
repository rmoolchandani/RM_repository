---
title: "Hierarchical Clustering"
author: "Rakhee Moolchandani"
date: "12/06/2020"
output: 
  pdf_document: default
  html_document: default
always_allow_html: true
---

## Assignment 5  
The purpose of this assignment is to use Hierarchical Clustering.  

For this project, we are going to use cereals dataset which includes nutritional information, store display, and consumer ratings for 77 breakfast cereals. For each cereal, there are 16 measurements, which are the following:

* Name: Name of cereal
* mfr: Manufacturer of cereal  
  + A = American Home Food Products;
  + G = General Mills
  + K = Kelloggs
  + N = Nabisco
  + P = Post
  + Q = Quaker Oats
  + R = Ralston Purina 
* type:
  + cold
  + hot 
* calories: calories per serving
* protein: grams of protein
* fat: grams of fat
* sodium: milligrams of sodium
* fiber: grams of dietary fiber
* carbo: grams of complex carbohydrates
* sugars: grams of sugars
* potass: milligrams of potassium
* vitamins: vitamins and minerals - 0, 25, or 100, indicating the typical percentage of FDA recommended
* shelf: display shelf (1, 2, or 3, counting from the floor)
* weight: weight in ounces of one serving
* cups: number of cups in one serving
* rating: a rating of the cereals


## Load the required libraries
```{r,message=FALSE}
library(readr) 
library(tidyverse) 
library(factoextra) 
library(psych) 
library(ggplot2) 
library(ggpubr) 
library(corrplot) 
library(RColorBrewer) 
library(data.table) 
library(caret)
library(DMwR)
library(dendextend)
library(cluster)
library(gridExtra)
```


## Reading and Understanding the Data
```{r}
# Load the file
Cereals <- read.csv("Cereals.csv")

# Show the first 6 rows 
head(Cereals)

# Show the last 6 rows 
tail(Cereals)
```


It is important to run the head and tail of the dataframe to confirm that the dataframe is similar among its data points.  

## Data Exploration and Visualization
```{r}
# To get the total number of rows and columns
dim(Cereals  )
```


This output shows that the Cereals data frame has 77 data points and 16 variables.

```{r}
# See the data frame structure
str(Cereals)
```


This data frame has 5 numerical variables such calories, protein, fat, etc. and 3 categorical variables, which are following: 
* Name  
* mfr  
* type


```{r}
# To get descriptive statistics
summary(Cereals) 
```


This help us to see some descriptive statistics and also to determine that variables carbo, sugars, potass have missing values.

```{r, warning=FALSE, message=FALSE}
# Lets visualize the data for each attribute
Cereals %>% gather(Attributes, value, 4:16) %>% ggplot(aes(x=value)) + geom_histogram(fill = "lightblue", color = "blue") + facet_wrap(~Attributes, scales = "free_x") + labs(x = "Value", y = "Frequency")
```


This allows us to visualize the statistical distribution of the variables. Also, it appears that the shelf column in categorical.


## Data Preparation
```{r}
# Lets save the original dataset before making any changes for future reference
MasterCereals <- Cereals
# Convert the names of the breakfast cereals to the row names, as this will later help us in visualizing the clusters
rownames(Cereals) <- Cereals$name
# Drop the name, mfr and type column as they are not used. Also, drop Shelf variable because it’s categorical
Cereals <- Cereals[,c(-1,-2,-3,-13)]
# Make sure the columns are dropped
head(Cereals)
```


This shows that the names of the breakfast cereals are converted to the row names and also, the name, type, mfr and shelf columns are dropped.


## Scaling or Normalization 
The data must be scaled, before measuring any type of distance metric as the variables with higher ranges will significantly influence the distance.
```{r}
# Normalize the data using the scale function 
Cereals <- scale(Cereals)

# See the first 6 rows 
head(Cereals)
```


**Remove missing values**  
Remember that normalizing the data first and then removing the missing values will help us to find the true mean and true standard deviation to accurately run the model.
```{r}
# Find the number of missing values
sum(is.na(Cereals))
```


There are 4 missing values in dataset. 
Lets remove them.
```{r}
# Remove missing values
Cereals <- na.omit(Cereals)
# To get the total number of rows and columns
dim(Cereals)
```


Here we can see that 3 rows were removed and we only have 12 variables.

## Correlation Matrix
What’s the relationship between the different scaled attributes? Use `corrplot()` to create correlation matrix.
```{r}
# Multivariate correlation matrix
corrplot(cor(Cereals), type = "lower",main="Correlation matrix", order = "hclust", mar=c(0,0,1,0),tl.cex=0.8,tl.col="black", col = brewer.pal(n = 8, name = "PuOr"))
```


So, this plot will help us to analyze how data is correlated and it might help us to get some insights. Here we see the following patterns:  
* There is a positive correlation between:  
Calories and Weight   
Calories and Fat  
Sugars and Calories  
* There is a negative correlation between:  
Potassium and Fiber  
Sugars and Rating  

## Computing Distance
For computing the distance, we are going to use the get_dist function from the factoextra package in R.
The get_dist() function computes a distance matrix between the rows of a data matrix and it uses the Euclidean distance as default.
```{r}
# Computing the distance
dist <- get_dist(Cereals)

# See the first 6 rows
head(dist)

#  Let’s visualize our distances. The fviz_dist() function visualizes a distance matrix
fviz_dist(dist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07",lab_size = 0.1), lab_size = 5)
```


This graph is a distance matrix. As we can see, the diagonal values with blue line are zeros because it is showing the distance between any point against itself. The orange represents the furthest distance between any pair of observations.


## Hierarchical Clustering
Let’s now perform hierarchical clustering using the hclust() function, for which we’ll first need to calculate the distance measures using Euclidean method which is already calculated above.
```{r}
# Run the hierarchical clustering using Ward method
hc_fit <- hclust(dist, method = "ward.D2")

# We can display the dendrogram for hierarchical clustering, using the plot() function
plot(hc_fit,cex = 0.6, hang = -1, main = "Hierarchical Clustering Dendrogram")
```


Based on this dendrogram, cluster size 6 seems appropriate.   
Let's visualize the 6 clusters.
```{r}
# Plot a new dendrogram, with each of the clusters being displayed in a different, using the A2Rplot() function
plot(hc_fit, cex = 0.6, hang = -1, main = "Hierarchical Clustering Dendrogram")
rect.hclust(hc_fit, k = 6, border = 2:10) 
```


We can see the 6 clusters are formed in the above dendrogram.

Add Cluster numbers to the dataframe.
```{r}
# Lets Cut the tree to 6 clusters, using the cutree() function
points_hc <- cutree(hc_fit, k = 6)

# Store the clusters in a data frame along with the cereals data
Cereals_Cluster <- cbind(points_hc, Cereals)

# Name the cluster number column as 'Cluster'
colnames(Cereals_Cluster)[1] <- "Cluster"

# Have a look at the head of the new data frame
head(Cereals_Cluster)
```


The Cluster column is added to the dataframe.


## AGNES 
Lets use Agnes to compare the clustering from single linkage, complete linkage, average linkage, and Ward methods and Choose the best method out of these.
```{r}
# Run agnes with single linkgae
hc_single <- agnes(Cereals, method = "single") 

# Run agnes with complete linkgae
hc_complete <- agnes(Cereals, method = "complete") 

# Run agnes with average linkgae
hc_average <- agnes(Cereals, method = "average") 

# Run agnes with ward method
hc_ward <- agnes(Cereals, method = 'ward')

# Compare Agglomerative Coefficients for each agnes method
m <- c( "single", "complete", "average", "ward") 
names(m) <- c( "Single", "Complete", "Average", "Ward")

# function to compute coefficients for all the methods
ac <- function(x) { agnes(Cereals, method = x)$ac}
map_dbl(m, ac)
```


The above table shows the Agglomerative coefficients for Single linkage, Complete linkage, Average linkage and Ward Methods. The Ward method has the highest vale of 0.9088 i.e. the accuracy of 91% and hence it is the best method.

## Optimal Clusters
**Lets run Elbow and Silhouette Methods to find the optimal values of K for Hierarchical clustering**
```{r}
# Elbow method
p1 <- fviz_nbclust(Cereals, FUN = hcut , method = "wss", k.max = 10) +
  ggtitle("(A) Elbow method") + geom_vline(xintercept = 6, linetype = 2)

# Silhouette Method
p2 <- fviz_nbclust(Cereals, FUN = hcut , method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

# Display plots side by side
grid.arrange(p1, p2, nrow = 1)
```


Here, we can see that the Elbow and Silhouette methods have different values. The Elbow Method shows k=6 whereas the Silhouette Method shows k=9 as the optimal value. Since our Cereal data structure is small, I am choosing k=6 as the optimal value of k to keep the clusters compact. 

Now, lets visualize the dendrogram of the best method which is ward
```{r}
fviz_dend(hc_ward, k = 6, main = "Dendrogram of Wards Method", cex = 0.6,
k_colors = c( "red", "blue", "orange", "purple", "green", "yellow"), color_labels_by_k = TRUE,
labels_track_height = 16, ggtheme = theme_bw())
```


## K-Means Clustering
* Lets now run the K-Means clustering and see the difference between the K-Means and the Hierarchical Clustering
We are going to run methods to choose our optimal k.

**Lets run Elbow and Silhouette Methods to find the optimal values of K for kmeans clustering**
```{r}
# Elbow method
p1 <- fviz_nbclust(Cereals, kmeans , method = "wss", k.max = 10) +
  ggtitle("(A) Elbow method") + geom_vline(xintercept = 6, linetype = 2)

# Silhouette Method
p2 <- fviz_nbclust(Cereals, kmeans , method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

# Display plots side by side
grid.arrange(p1, p2, nrow = 1)
```


Again, the above graphs show the different values. The Elbow Method shows k=6 whereas the Silhouette Method shows k=10 as the optimal value. I am choosing the same value k=6 as the optimal value of k again.

Run the kmeans
```{r}
# Set seed for reproducibility
set.seed(123)

# Run kmeans model
km_clust <- kmeans(Cereals, 6, nstart = 10)

# See the size of clusters
km_clust$size

# Visualize the Clusters
fviz_cluster(km_clust, Cereals, main = "K-means Clustering", labelsize = 6)
```


## Difference between Hierarchical and K-means clustering
In hierarchical clustering, observations are sequentially grouped to create clusters, based on distances between observations and distances between clusters.It also produces a useful graphical display of the clustering process and results, called a dendrogram.In k -means clustering, observations are allocated to one of a pre-specified set of clusters, according to their distance from each cluster.


## Compare Hierarchical and K-means clustering
```{r}
# Following table shows the number of observations in each clusters for Hierarchical clustering
table(points_hc, dnn = "Hierarchical Clusters")
# Following table shows the number of observations in each clusters for kmeans clustering
table(km_clust$cluster, dnn = "k-means Clusters")

# Lets visualize both the clustering methods
PlotCereals <- na.omit(MasterCereals)

# Plot of hierarchical clustering clusters
pl1 <- ggplot(data = PlotCereals, aes(points_hc)) + geom_bar(fill = "blue4") +
labs(title="Clusters Assignment - Hierarchical") + labs(x="Cluster Assignment", y="Count") +
guides(fill=FALSE) +
scale_x_continuous(breaks=c(1:6)) + scale_y_continuous(breaks=c(5,10,15,20), limits = c(0,25))

# Plot of K-means clustering clusters
pl2 <- ggplot(data = PlotCereals, aes(km_clust$cluster)) + geom_bar(fill = "blue4") +
labs(title="Clusters Assignment - k-means") + labs(x="Cluster Assignment", y="Count") +
guides(fill=FALSE) +
scale_x_continuous(breaks=c(1:6)) + scale_y_continuous(breaks=c(5,10,15,20), limits = c(0,25))

# plot the graphs for both the clustering methods side by side
grid.arrange(pl1, pl2, nrow = 1)
```


The above table and display shows the number of data points per cluster in Hierarchical and kmeans clustering respectively.

Lets see the cluster of each observation in both the clustering methods
```{r}
# Store the clusters in a data frame along with the cereals data
Cereals_Cluster <- cbind(points_hc, km_clust$cluster, PlotCereals[c(0)])

# Name the cluster number column as 'Cluster'
colnames(Cereals_Cluster)[1] <- "H Cluster"
colnames(Cereals_Cluster)[2] <- "kmeans Cluster"

D1 <- filter(Cereals_Cluster, `H Cluster`==1)
D2 <- filter(Cereals_Cluster, `H Cluster`==2)
D3 <- filter(Cereals_Cluster, `H Cluster`==3)
D4 <- filter(Cereals_Cluster, `H Cluster`==4)
D5 <- filter(Cereals_Cluster, `H Cluster`==5)
D6 <- filter(Cereals_Cluster, `H Cluster`==6)

rbind(D1, D2, D3, D4, D5, D6)
```


On comparing the k-means clusters with that of the Hierarchical clusters, we can say that the optimal number of clusters are quite same in both the methods.   
By looking at both plots, the clusters were classified very similar. By looking at the each observation, we can figure out following  
* Cluster 1 in hierarchical is similar to cluster 2 in kmeans. And by the table we can confirm that both have same 3 cereals in it.  
* Similarly, cluster 3 from hierarchical is similar to cluster 6 from kmeans and vice versa. Again, both have same cereals.  
* Cluster 4 is exactly similar in both the clustering methods.  
* Therefore, we can say that the cluster similarity is almost 74%.  

**Final thoughts and advantages of hierarchical clustering compared to k-means:**  
Hierarchical clustering may have some benefits over k-means such as not having to pre-specify the number of clusters and the fact that it can produce a nice hierarchical illustration of the clusters (that’s useful for smaller data sets). However, from a practical perspective, hierarchical clustering analysis still involves a number of decisions that can have large impacts on the interpretation of the results.  
First, like k-means, you still need to make a decision on the dissimilarity measure to use.  
Second, you need to make a decision on the linkage method. Each linkage method has different systematic tendencies (or biases) in the way it groups observations and can result in significantly different results.  
Third, although we do not need to pre-specify the number of clusters, we often still need to decide where to cut the dendrogram in order to obtain the final clusters to use. So it is still on us to decide the number of clusters.  


## Cluster Stability
To check the stability of clusters, run the heirachical clustering on Partitioned data set and check the structure of datasets  

Partition the data set
```{r}
# Set seed for reproducibility
set.seed(123)

# Partition the data set
Cereals_index <- createDataPartition(Cereals[,'protein'], p=0.75, list=FALSE)

# Partition A : Train data, Partition B: Test Data
train_data<-Cereals[Cereals_index,] # Partition A
test_data<-Cereals[-Cereals_index,] # Partition B

# Run all the modes and find the best model 
hc11<- agnes(train_data,method = "ward") 
hc12<-agnes(train_data,method="average") 
hc13<-agnes(train_data,method="complete") 
hc14<-agnes(train_data,method="single")

cbind(ward=hc11$ac,average=hc12$ac,complete=hc13$ac,single=hc14$ac)
```


Again, we can see that the ward method is the best method.


Run the Hierarchical model using Ward method
```{r}
# Calculate the distance using the Euclidean method
Newdist <- dist(train_data, method = "euclidean")

# Run Hierarchical clustering using the distance calculated
hc_fit1 <- hclust(Newdist, method = "ward.D2")

#We can display the dendrogram for hierarchical clustering, using the plot() function
plot(hc_fit1,cex = 0.6, hang = -1)
rect.hclust(hc_fit1, k = 6, border = 2:4)
```

```{r}
# Cut the tree into 6 clusters for analysis
clust2<-cutree(hc_fit1, k=6)

# Add the assigned cluster to the pre-processed data set
result<-as.data.frame(cbind(train_data,clust2))

# Determine centroids for all 6 clusters
centroid1<-data.frame(column=seq(1,12,1),mean=rep(0,12)) 
centroid2<-data.frame(column=seq(1,12,1),mean=rep(0,12)) 
centroid3<-data.frame(column=seq(1,12,1),mean=rep(0,12)) 
centroid4<-data.frame(column=seq(1,12,1),mean=rep(0,12)) 
centroid5<-data.frame(column=seq(1,12,1),mean=rep(0,12)) 
centroid6<-data.frame(column=seq(1,12,1),mean=rep(0,12)) 
for(i in 1:12)
{
centroid1[i,2]<-mean(result[result$clust2==1,i]) 
centroid2[i,2]<-mean(result[result$clust2==2,i]) 
centroid3[i,2]<-mean(result[result$clust2==3,i]) 
centroid4[i,2]<-mean(result[result$clust2==4,i])
centroid5[i,2]<-mean(result[result$clust2==5,i]) 
centroid6[i,2]<-mean(result[result$clust2==6,i]) 
} 

# Combine the centroids of all the variables
centroidResult<-t(cbind(centroid1$mean,centroid2$mean,centroid3$mean,centroid4$mean,centroid5$mean,centroid6$mean))
colnames(centroidResult)<-colnames(Cereals)

# Assign the clusters to the test data based on the minimum distance to cluster centers
Dumm1 <- data.frame(data=seq(1,17,1), cluster=rep(0,17)) 
for(i in 1:17)
{
R <- as.data.frame(rbind(centroidResult,test_data[i,])) 
U <- as.matrix(get_dist(R))
Dumm1[i,2] <- which.min(U[7,-7])
} 

# Combine partitions for comparison to original clusters
result1<-as.data.frame(cbind(test_data,Dumm1$cluster))
colnames(result1)[13] <- "clust2"
Finalresult <- rbind(result,result1)

# Compare the number of matching assignments to see the stability of the clusters.
table(points_hc == Finalresult$clust2)
```


From this result, it can be stated that the clusters are not very stable. With 75% of the data available, the resulting assignments were only identical for 26 out of the 74 observations. This results in a 35% similarity of cluster assignment.


**Find the cluster of Healthy Cereals**  
In this case, normalizing the data would not be appropriate because the normalizing of the cereal nutritional information is based on the sample of cereal being analyzed. Therefore, the gathered dataset could include only cereals with very high sugar content and very low fiber, iron, and other nutritional information. Once it is normalized across the sample set, it is impossible to state how much nutrition the cereal will give a child.  

## Healthy Cereal Cluster
```{r}
# Read the dataset and remove any missing values and add the cluster column
Cereals_data <- MasterCereals
Cereals_data <-na.omit(Cereals_data) 
Cereals_data<-cbind(Cereals_data,points_hc) 

# Lets compare all the variables in each cluster
# cluster vs calories
plot1 <- Cereals_data %>% 
    ggplot(aes(x = points_hc, y = calories)) + 
    geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "blue") +
  labs(x = "Clusters", y="Calories")

# cluster vs Protein  
plot2 <-  Cereals_data %>%
  ggplot(aes(x = points_hc, y = protein)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "orange") +
  labs(x = "Clusters", y="Protein")

# cluster vs fat
plot3 <-  Cereals_data %>%
  ggplot(aes(x = points_hc, y = fat)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "green") +
  labs(x = "Clusters", y="Fat")

# cluster vs sodium
plot4 <-  Cereals_data %>%
  ggplot(aes(x = points_hc, y = sodium)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "yellow") +
  labs(x = "Clusters", y="Sodium")

# cluster vs fiber
plot5 <-  Cereals_data %>%
  ggplot(aes(x = points_hc, y = fiber)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "red") +
  labs(x = "Clusters", y="Fiber")

# cluster vs carbs
plot6 <-  Cereals_data %>%
  ggplot(aes(x = points_hc, y = carbo)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "brown") +
  labs(x = "Clusters", y="Carbohydrates")

# cluster vs sugar
plot7 <-  Cereals_data %>%
  ggplot(aes(x = points_hc, y = sugars)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "pink") +
  labs(x = "Clusters", y="Sugars")

# cluster vs potassium
plot8 <-  Cereals_data %>%
  ggplot(aes(x = points_hc, y = potass)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "purple") +
  labs(x = "Clusters", y="Potassium")

# cluster vs vitamins
plot9 <-  Cereals_data %>%
  ggplot(aes(x = points_hc, y = vitamins)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "orange") +
  labs(x = "Clusters", y="Vitamins")

# plot the graphs side by side
grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9)
```


This output shows that the Cluster 1 has set of cereals which have less calories, less fat, more fiber, less carbs, less sugars as compared to other clusters. This cluster looks healthy option. Lets also check Which nutrients are essential for a nutritious breakfast per rating.


```{r}
# Lets see the relation between ratings and all the nutrients
par(mfcol=c(3,3))

# rating vs calories
plot(Cereals_data$rating~calories, 
     data = Cereals_data, 
     xlab="Calories", 
     ylab="Rating", 
     main="Rating vs Calories",
     col="blue")
abline(lm(Cereals_data$rating~Cereals_data$calories), col="red")

# rating vs protein
plot(Cereals_data$rating~protein, 
     data = Cereals_data, 
     xlab="Protein", 
     ylab="Rating", 
     main="Rating vs Protein",
     col="blue")
abline(lm(Cereals_data$rating~Cereals_data$protein), col="red")

# rating vs fat
plot(Cereals_data$rating~fat, 
     data = Cereals_data, 
     xlab="Fat", 
     ylab="Rating", 
     main="Rating vs Fat",
     col="blue")
abline(lm(Cereals_data$rating~Cereals_data$fat), col="red")

# rating vs sodium
plot(Cereals_data$rating~sodium, 
     data = Cereals_data, 
     xlab="Sodium", 
     ylab="Rating", 
     main="Rating vs Sodium",
     col="blue")
abline(lm(Cereals_data$rating~Cereals_data$sodium), col="red")

# rating vs fiber
plot(Cereals_data$rating~fiber, 
     data = Cereals_data, 
     xlab="Fiber", 
     ylab="Rating", 
     main="Rating vs Fiber",
     col="blue")
abline(lm(Cereals_data$rating~Cereals_data$fiber), col="red")

# rating vs carbo
plot(Cereals_data$rating~carbo, 
     data = Cereals_data, 
     xlab="Carbohydrates", 
     ylab="Rating", 
     main="Rating vs Carbohydrates",
     col="blue")
abline(lm(Cereals_data$rating~Cereals_data$carbo), col="red")

# rating vs sugars
plot(Cereals_data$rating~sugars, 
     data = Cereals_data, 
     xlab="Sugar", 
     ylab="Rating", 
     main="Rating vs Sugar",
     col="blue")
abline(lm(Cereals_data$rating~Cereals_data$sugars), col="red")

# rating vs potassium
plot(Cereals_data$rating~potass, 
     data = Cereals_data, 
     xlab="Potassium", 
     ylab="Rating", 
     main="Rating vs Potassium",
     col="blue")
abline(lm(Cereals_data$rating~Cereals_data$potass), col="red")

# rating vs vitamins
plot(Cereals_data$rating~vitamins, 
     data = Cereals_data, 
     xlab="Vitamins", 
     ylab="Rating", 
     main="Rating vs Vitamins",
     col="blue")
abline(lm(Cereals_data$rating~Cereals_data$vitamins), col="red")
```


We can see that the rating is dependent on the nutrients of the cereals. For example:  
* As the calories, sodium, sugar, fat content increases, the rating decreases.  
* As the protein, fiber content increases, the rating also increases.  
Therefore, we can say that there is significance of rating with reagrds to clusters.  


```{r}
# Lets see the cluster with the highest rating
boxplot(rating~points_hc,
        data = Cereals_data,
        xlab = "Clusters",
        ylab = "Ratings",
        main = "Rating vs Clusters",
        col = topo.colors(7))
```


Here also,from the above graph it appears that the Cluster 1 has highest ratings. So Cluster 1 is a cluster of “Healthy Cereals”
